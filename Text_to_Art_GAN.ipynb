{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNsK-Xhh6S0c"
      },
      "source": [
        "#Using CLIP and VQGAN Models to Generate Images from Text Prompts\n",
        "\n",
        "In this notebook, we will the CLIP and VQGAN models to generate paintings from text prompts. We will then prepare this notebook to be used as an API for generating art on demand.\n",
        "\n",
        "## Acknowledgements\n",
        "The original notebook was made by [Katherine Crowson](https://github.com/crowsonkb).\n",
        "\n",
        "With further modifications by [Justin John](https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP_%28z%2Bquantize_method_with_augmentations%2C_user_friendly_interface%29.ipynb#scrollTo=c3d7a8be-73ce-4cee-be70-e21c1210a7a6).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, double checking the type of GPU that the notebook is using a GPU  since this notebook require significant computational power. <br> \n",
        "Depending on your Colab subscription, you will be likely getting different GPUs:\n",
        "\n",
        "(slowest, not recommended) **P4 << K80 << T4 << P100 << V100** (fastest)\n"
      ],
      "metadata": {
        "id": "nyQdLFGx7VwS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJASB6EqlqNm"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7q5N1TeO-Wb"
      },
      "source": [
        "## Initialize the System"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the needed dependencies, we will be downloading the weights pretrained on the wikiart dataset."
      ],
      "metadata": {
        "id": "wyBfe047DGjP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXMSuW2EQWsd"
      },
      "outputs": [],
      "source": [
        "# Initialize the System\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!nvidia-smi\n",
        "print(\"Downloading CLIP...\")\n",
        "!git clone https://github.com/openai/CLIP                 &> /dev/null\n",
        " \n",
        "print(\"Installing Python Libraries for AI\")\n",
        "!git clone https://github.com/CompVis/taming-transformers &> /dev/null\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning  &> /dev/null\n",
        "!pip install kornia                                       &> /dev/null\n",
        "!pip install einops                                       &> /dev/null\n",
        "print(\"Installing transformers library...\")\n",
        "!pip install transformers   \n",
        "print(\"Installing taming.models...\")   \n",
        "!pip install taming.models                           &> /dev/null\n",
        "\n",
        "print(\"Installing Python Libraries for API Dev\")\n",
        "!pip install pycurl fastapi uvicorn nest-asyncio pyngrok python-multipart py_eureka_client\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload                  &> /dev/null\n",
        "\n",
        "!curl -L  'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml' > wikiart_16384.yaml\n",
        "!curl -L  'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt' > wikiart_16384.ckpt\n",
        "print(\"Installation finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up needed libraries and methods:"
      ],
      "metadata": {
        "id": "jx_4vyU0GsQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        " \n",
        "sys.path.append('./taming-transformers')\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        " \n",
        "from CLIP import clip\n",
        "import kornia.augmentation as K\n",
        "import numpy as np\n",
        "import imageio\n",
        "from PIL import ImageFile, Image\n",
        "import json\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        " \n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        " \n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        " \n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        " \n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        " \n",
        "    input = input.view([n * c, 1, h, w])\n",
        " \n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        " \n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        " \n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        " \n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        " \n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        " \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        " \n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        " \n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        " \n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        " \n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        " \n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.augs = nn.Sequential(\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
        "            K.RandomSharpness(0.3,p=0.4),\n",
        "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
        "            K.RandomPerspective(0.2,p=0.4),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n",
        "        self.noise_fac = 0.1\n",
        " \n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        " \n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        " \n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "!mkdir steps\n",
        "!mkdir output\n",
        "!mkdir /content/current"
      ],
      "metadata": {
        "id": "C7wSoV8gC6oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTjLpt0BPnft"
      },
      "source": [
        "##  Prepare the Arguments\n",
        "This is the main method to prepare the arguments needed to generate art.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPndMUlCYLjN"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "def prep_args(prompts = \"rainbow sunflowers\",\n",
        "              width =  64,\n",
        "              height =  64,\n",
        "              display_frequency =  25,\n",
        "              initial_image = \"\",\n",
        "              target_images = \"\",\n",
        "              learning_rate = 0.1,\n",
        "              max_iterations = 100,\n",
        "              input_images = \"\"):\n",
        "  \n",
        "  seed = None\n",
        "  if initial_image == \"None\":\n",
        "      initial_image = None\n",
        "  if target_images == \"None\" or not target_images:\n",
        "      target_images = []\n",
        "  else:\n",
        "      target_images = target_images.split(\"|\")\n",
        "      target_images = [image.strip() for image in target_images]\n",
        "\n",
        "  if initial_image or target_images != []:\n",
        "      input_images = True\n",
        "\n",
        "  prompts = [phrase.strip() for phrase in prompts.split(\"|\")]\n",
        "  if prompts == ['']:\n",
        "      prompts = []\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  print('Using device:', device)\n",
        "  if prompts:\n",
        "      print('Using text prompt:', prompts)\n",
        "  if target_images:\n",
        "      print('Using image prompts:', target_images)\n",
        "\n",
        "  args = argparse.Namespace(\n",
        "      prompts=prompts,\n",
        "      image_prompts=target_images,\n",
        "      noise_prompt_seeds=[],\n",
        "      noise_prompt_weights=[],\n",
        "      size=[width, height],\n",
        "      init_image=initial_image,\n",
        "      init_weight=0.,\n",
        "      clip_model='ViT-B/32',\n",
        "      step_size=learning_rate,\n",
        "      cutn=64,\n",
        "      cut_pow=1.,\n",
        "      display_freq=display_frequency,\n",
        "      seed=seed,\n",
        "    vqgan_config='wikiart_16384.yaml',         \n",
        "      vqgan_checkpoint='wikiart_16384.ckpt',\n",
        "      device=device,\n",
        "      max_iterations=max_iterations)\n",
        "  return args"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Text-to-Image Generator\n",
        "This is the main code for the Art generator"
      ],
      "metadata": {
        "id": "rzJflsS1G16Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uOVCRugPj5n"
      },
      "outputs": [],
      "source": [
        "class Text2ImageGenerator():\n",
        "  def __init__(self,args):\n",
        "    torch.cuda.empty_cache()\n",
        "    if args.seed is None:\n",
        "        self.seed = torch.seed()\n",
        "    else:\n",
        "        self.seed = args.seed\n",
        "    torch.manual_seed(self.seed)\n",
        "    print('Using seed:', self.seed)\n",
        "    self.max_iterations=args.max_iterations\n",
        "    self.init_weight=args.init_weight\n",
        "    self.display_freq=args.display_freq\n",
        "    self.model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(args.device)\n",
        "    self.perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(args.device)\n",
        "\n",
        "    self.cut_size = self.perceptor.visual.input_resolution\n",
        "    self.e_dim = self.model.quantize.e_dim\n",
        "    self.f = 2**(self.model.decoder.num_resolutions - 1)\n",
        "    self.make_cutouts = MakeCutouts(self.cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "    self.n_toks = self.model.quantize.n_e\n",
        "    self.toksX, self.toksY = args.size[0] // self.f, args.size[1] // self.f\n",
        "    self.sideX, self.sideY = self.toksX * self.f, self.toksY * self.f\n",
        "    self.z_min = self.model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    self.z_max = self.model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "\n",
        "    if args.init_image:\n",
        "        self.pil_image = Image.open(args.init_image).convert('RGB')\n",
        "        self.pil_image = self.pil_image.resize((self.sideX, self.sideY), Image.LANCZOS)\n",
        "        self.z, *_ = self.model.encode(TF.to_tensor(self.pil_image).to(args.device).unsqueeze(0) * 2 - 1)\n",
        "    else:\n",
        "        self.one_hot = F.one_hot(torch.randint(self.n_toks, [self.toksY * self.toksX], device=args.device), self.n_toks).float()\n",
        "        self.z = self.one_hot @ self.model.quantize.embedding.weight\n",
        "        self.z = self.z.view([-1, self.toksY, self.toksX, self.e_dim]).permute(0, 3, 1, 2)\n",
        "    self.z_orig = self.z.clone()\n",
        "    self.z.requires_grad_(True)\n",
        "    self.opt = optim.Adam([self.z], lr=args.step_size)\n",
        "\n",
        "    self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                  std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "    self.pMs = []\n",
        "\n",
        "    for prompt in args.prompts:\n",
        "        txt, weight, stop = parse_prompt(prompt)\n",
        "        embed = self.perceptor.encode_text(clip.tokenize(txt).to(args.device)).float()\n",
        "        self.pMs.append(Prompt(embed, weight, stop).to(args.device))\n",
        "\n",
        "    for prompt in args.image_prompts:\n",
        "        path, weight, stop = parse_prompt(prompt)\n",
        "        img = resize_image(Image.open(path).convert('RGB'), (self.sideX, self.sideY))\n",
        "        batch = self.make_cutouts(TF.to_tensor(img).unsqueeze(0).to(args.device))\n",
        "        embed = self.perceptor.encode_image(self.normalize(batch)).float()\n",
        "        self.pMs.append(Prompt(embed, weight, stop).to(args.device))\n",
        "\n",
        "    for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "        gen = torch.Generator().manual_seed(seed)\n",
        "        embed = torch.empty([1, self.perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "        self.pMs.append(Prompt(embed, weight).to(args.device))\n",
        "\n",
        "  def synth(self):\n",
        "      self.z_q = vector_quantize(self.z.movedim(1, 3), self.model.quantize.embedding.weight).movedim(3, 1)\n",
        "      return clamp_with_grad(self.model.decode(self.z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def checkin(self):\n",
        "      losses_str = ', '.join(f'{loss.item():g}' for loss in self.losses)\n",
        "      tqdm.write(f'i: {self.i}, loss: {sum(self.losses).item():g}, losses: {losses_str}')\n",
        "      out = self.synth()\n",
        "      TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "      display.display(display.Image('progress.png'))\n",
        "\n",
        "  def ascend_txt(self):\n",
        "      out = self.synth()\n",
        "      iii = self.perceptor.encode_image(self.normalize(self.make_cutouts(out))).float()\n",
        "      result = []\n",
        "      if self.init_weight:\n",
        "          result.append(F.mse_loss(self.z, self.z_orig) * self.init_weight / 2)\n",
        "      for prompt in self.pMs:\n",
        "          result.append(prompt(iii))\n",
        "      img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "      img = np.transpose(img, (1, 2, 0))\n",
        "      filename = f\"steps/{self.i:04}.png\"\n",
        "      imageio.imwrite(filename, np.array(img))\n",
        "      imageio.imwrite(\"/content/current/current.png\", np.array(img))\n",
        "      with open(\"/content/current/current.txt\",\"w\") as f:\n",
        "        f.write(str(self.i))\n",
        "\n",
        "      return result\n",
        "\n",
        "  def train(self):\n",
        "      self.opt.zero_grad()\n",
        "      self.losses = self.ascend_txt()\n",
        "      if self.i % self.display_freq == 0:\n",
        "          self.checkin()\n",
        "      loss = sum(self.losses)\n",
        "      loss.backward()\n",
        "      self.opt.step()\n",
        "      with torch.no_grad():\n",
        "          self.z.copy_(self.z.maximum(self.z_min).minimum(self.z_max))\n",
        "  \n",
        "  def generate(self):\n",
        "    self.i = 0\n",
        "    try:\n",
        "        with tqdm() as pbar:\n",
        "            while True:\n",
        "                self.train()\n",
        "                if self.i == self.max_iterations:\n",
        "                    break\n",
        "                self.i += 1\n",
        "                status=self.i\n",
        "                print(self.i)\n",
        "                pbar.update()\n",
        "                #yield from open(f\"steps/{self.i:04}.png\",'rb')\n",
        "    except KeyboardInterrupt:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1jj_lhDPNNm"
      },
      "source": [
        "## Preparing the API\n",
        "In order to run the API from colab, you will need to authenticate  pyngrok with your own token. Make sure to create an account and copy the token below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC_I8BihtT6-"
      },
      "outputs": [],
      "source": [
        "!pyngrok  authtoken \"ADD_YOUR_TOKEN_HERE\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's build the Fast API application:"
      ],
      "metadata": {
        "id": "HkWrLdbdNw8L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQUmQsG4vBmM"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi import FastAPI, File, UploadFile, Form, BackgroundTasks\n",
        "from fastapi.responses import StreamingResponse\n",
        "from io import BytesIO\n",
        "import asyncio\n",
        "from http import HTTPStatus\n",
        "import os\n",
        "from py_eureka_client import eureka_client\n",
        "\n",
        "app = FastAPI()\n",
        "rest_server_port=8000\n",
        "eureka_client.init(eureka_server=\"https://artist-block-discovery-service.herokuapp.com/eureka\",\n",
        "                  app_name=\"GAN-Model\",\n",
        "                   instance_port=rest_server_port)\n",
        "\n",
        "\n",
        "origins = [\"*\"]\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define now the endpoints we will be exposing in our API:"
      ],
      "metadata": {
        "id": "IIq_A6ENN79A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80lSTTCevP_s"
      },
      "outputs": [],
      "source": [
        "from starlette.concurrency import run_in_threadpool\n",
        "\n",
        "@app.post('/v1/generate_text2img')\n",
        "async def generate_text2img(prompt:str,width:int,height:int):\n",
        "  args=prep_args(prompts=prompt,width=width,height=height,max_iterations=100)\n",
        "  gen_obj=Text2ImageGenerator(args)\n",
        "  imarray = np.random.rand(width,height,3) * 255\n",
        "  im = Image.fromarray(imarray.astype('uint8')).convert('RGB')\n",
        "  im.save('/content/current/current.png')\n",
        "  await run_in_threadpool(gen_obj.generate)\n",
        "  output_image = Image.open(\"/content/steps/0100.png\")\n",
        "  sent_image = BytesIO()\n",
        "  output_image.save(sent_image, \"JPEG\")\n",
        "  sent_image.seek(0)\n",
        "  return StreamingResponse(sent_image, media_type=\"image/jpeg\")\n",
        "\n",
        "@app.post('/v1/generate_textimg2img')\n",
        "async def generate_text2img(prompt:str,width:int,height:int,image: UploadFile=File(...)):\n",
        "  args=prep_args(prompts=prompt,initial_image=image.file,width=width,height=height,max_iterations=100)\n",
        "  gen_obj=Text2ImageGenerator(args)\n",
        "  imarray = np.random.rand(width,height,3) * 255\n",
        "  im = Image.fromarray(imarray.astype('uint8')).convert('RGB')\n",
        "  im.save('/content/current/current.png')\n",
        "  await run_in_threadpool(gen_obj.generate)\n",
        "  output_image = Image.open(\"/content/steps/0100.png\")\n",
        "  sent_image = BytesIO()\n",
        "  output_image.save(sent_image, \"JPEG\")\n",
        "  sent_image.seek(0)\n",
        "  return StreamingResponse(sent_image, media_type=\"image/jpeg\")\n",
        "\n",
        "\n",
        "@app.post('/v1/generate_text2img_progressimg')\n",
        "async def generate_text2img_progressimgs():\n",
        "  output_image = Image.open(\"/content/current/current.png\")\n",
        "  sent_image = BytesIO()\n",
        "  output_image.save(sent_image, \"JPEG\")\n",
        "  sent_image.seek(0)\n",
        "  return StreamingResponse(sent_image, media_type=\"image/jpeg\")\n",
        "\n",
        "@app.post('/v1/generate_text2img_progressbar')\n",
        "async def generate_text2img_progressbar():\n",
        "  with open(\"/content/current/current.txt\",\"r\") as f:\n",
        "        i=f.read()\n",
        "  return i\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to set up the API:"
      ],
      "metadata": {
        "id": "TEmCLAF1PdK5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkN3iSn1Ofpu"
      },
      "outputs": [],
      "source": [
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "print('Public URL docs:', os.path.join(ngrok_tunnel.public_url,\"docs\"))\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PA0QnthPGVs"
      },
      "source": [
        "## Testing The Model\n",
        "We can even test the model here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7EDme5RYCrt"
      },
      "outputs": [],
      "source": [
        "#@title Generate an Image\n",
        "import argparse\n",
        "prompts = \"flowering magenta orchids in a rainy day impressionist\" #@param {type:\"string\"}\n",
        "width =  32#@param {type:\"number\"}\n",
        "height =  32#@param {type:\"number\"}\n",
        "display_frequency =  25#@param {type:\"number\"}\n",
        "initial_image = \"\"#@param {type:\"string\"}\n",
        "target_images = \"\"#@param {type:\"string\"}\n",
        "learning_rate = 0.1 #@param {type:\"slider\", min:0.01, max:1.0, step:0.01}\n",
        "max_iterations = 200#@param {type:\"number\"}\n",
        "input_images = \"\"\n",
        "\n",
        "seed = None\n",
        "if initial_image == \"None\":\n",
        "    initial_image = None\n",
        "if target_images == \"None\" or not target_images:\n",
        "    target_images = []\n",
        "else:\n",
        "    target_images = target_images.split(\"|\")\n",
        "    target_images = [image.strip() for image in target_images]\n",
        "\n",
        "if initial_image or target_images != []:\n",
        "    input_images = True\n",
        "\n",
        "prompts = [frase.strip() for frase in prompts.split(\"|\")]\n",
        "if prompts == ['']:\n",
        "    prompts = []\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    prompts=prompts,\n",
        "    image_prompts=target_images,\n",
        "    noise_prompt_seeds=[],\n",
        "    noise_prompt_weights=[],\n",
        "    size=[width, height],\n",
        "    init_image=initial_image,\n",
        "    init_weight=0.,\n",
        "    clip_model='ViT-B/32',\n",
        "    step_size=learning_rate,\n",
        "    cutn=64,\n",
        "    cut_pow=1.,\n",
        "    display_freq=display_frequency,\n",
        "    seed=seed,\n",
        "  vqgan_config='wikiart_16384.yaml',         \n",
        "    vqgan_checkpoint='wikiart_16384.ckpt'\n",
        "    )\n",
        "\n",
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "if prompts:\n",
        "    print('Using text prompt:', prompts)\n",
        "if target_images:\n",
        "    print('Using image prompts:', target_images)\n",
        "if args.seed is None:\n",
        "    seed = torch.seed()\n",
        "else:\n",
        "    seed = args.seed\n",
        "torch.manual_seed(seed)\n",
        "print('Using seed:', seed)\n",
        "\n",
        "#model = torch.load(\"g4nshAr31w4tErwEI6hTz.ckpt\").to(device)\n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "\n",
        "cut_size = perceptor.visual.input_resolution\n",
        "e_dim = model.quantize.e_dim\n",
        "f = 2**(model.decoder.num_resolutions - 1)\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "n_toks = model.quantize.n_e\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "sideX, sideY = toksX * f, toksY * f\n",
        "z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "\n",
        "if args.init_image:\n",
        "    pil_image = Image.open(args.init_image).convert('RGB')\n",
        "    pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n",
        "    z, *_ = model.encode(TF.to_tensor(pil_image).to(device).unsqueeze(0) * 2 - 1)\n",
        "else:\n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "    z = one_hot @ model.quantize.embedding.weight\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "z_orig = z.clone()\n",
        "z.requires_grad_(True)\n",
        "opt = optim.Adam([z], lr=args.step_size)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "pMs = []\n",
        "\n",
        "for prompt in args.prompts:\n",
        "    txt, weight, stop = parse_prompt(prompt)\n",
        "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "for prompt in args.image_prompts:\n",
        "    path, weight, stop = parse_prompt(prompt)\n",
        "    img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n",
        "    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "    embed = perceptor.encode_image(normalize(batch)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "    pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "def synth(z):\n",
        "    z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "    # print(\"\\n\", z_q.shape)\n",
        "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def checkin(i, losses):\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "    out = synth(z)\n",
        "    TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "    display.display(display.Image('progress.png'))\n",
        "\n",
        "def ascend_txt():\n",
        "    global i\n",
        "    out = synth(z)\n",
        "    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if args.init_weight:\n",
        "        result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "\n",
        "    for prompt in pMs:\n",
        "        result.append(prompt(iii))\n",
        "    img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    filename = f\"steps/{i:04}.png\"\n",
        "    imageio.imwrite(filename, np.array(img))\n",
        "    return result\n",
        "\n",
        "def train(i):\n",
        "    opt.zero_grad()\n",
        "    lossAll = ascend_txt()\n",
        "    if i % args.display_freq == 0:\n",
        "        checkin(i, lossAll)\n",
        "    loss = sum(lossAll)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    with torch.no_grad():\n",
        "        z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "i = 0\n",
        "try:\n",
        "    with tqdm() as pbar:\n",
        "        while True:\n",
        "            train(i)\n",
        "            if i == max_iterations:\n",
        "                break\n",
        "            i += 1\n",
        "            pbar.update()\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n",
        "print(\"Not for sale.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Text to Art GAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}